# CSE573-Fake Review Detection Group 6
CSE573 Fake Review Detection

# Contributors
<ol>
<li>Aditya Sinha</li>
<li>Praveenaa Kulandhaivel</li>
<li>Arjun Ramesh</li>
<li>Haritha Athinaryanan</li>
<li>Lakshmi Narayana Rao Boyapati</li>
<li>Junaita Davakumar</li>
</ol>


# Dataset

The generated fake reviews dataset, containing 20k fake reviews and 20k real product reviews. OR = Original reviews (presumably human created and authentic); CG = Computer-generated fake reviews.
More about the dataset:
https://osf.io/tyue9/

# Folder Structure
In this specific folder structure, each of the three folders - 'code', 'data', and 'evaluations' - has a distinct and crucial role in organizing the components of the project. The 'code' folder is designated for all Python files, encompassing the entire programming aspect of the project. This include Python scripts for data processing, analysis, model training, and any other computational tasks. 

The 'data' folder is exclusively used for storing the Amazon dataset. This dedicated space for the dataset ensures easy accessibility and management, particularly important for large datasets or when multiple datasets are involved.

Lastly, the 'evaluation' folder is critical for housing the evaluations of the results. This could include performance metrics, comparative analyses, visualizations, or detailed reports that assess the effectiveness, accuracy, and other aspects of the outputs generated by the Python scripts. By storing these evaluations separately, the structure allows for a clear distinction between the raw data, the processing code, and the analysis of the outcomes, thereby maintaining a high degree of organization and clarity in the project. 
# Ensemble Learning
## Bagging with Random Forest
Bagging with Random Forest is a powerful ensemble machine learning technique that combines the concepts of bootstrap aggregating (bagging) and the random forest algorithm to enhance model accuracy and robustness. In this approach, multiple decision trees are constructed during the training process. Each tree is built on a different bootstrap sample, which is a random subset of the training data, selected with replacement. This method of resampling introduces diversity in the data seen by each tree, thereby reducing the variance of the final model. Additionally, Random Forest introduces further randomness by selecting a random subset of features for splitting nodes in each decision tree, rather than considering all features. This randomness ensures that the individual trees in the forest are decorrelated, making the ensemble less sensitive to the specifics of the training data and less prone to overfitting. The final prediction of the Random Forest model is typically the average of the predictions from all trees for regression tasks, or a majority vote for classification tasks. This aggregation helps in achieving higher accuracy than individual decision trees and makes Bagging with Random Forest a robust choice for a wide range of applications.

| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 75.67%   |   75.67%  | 75.67% |  75.67%  |

## Boosting with ADA Boost
Boosting with AdaBoost (Adaptive Boosting) is a potent ensemble technique in machine learning that combines multiple weak learners, typically simple decision trees, to form a stronger predictive model. Unlike bagging methods like Random Forest, where trees are built independently, AdaBoost works by sequentially adding trees, where each subsequent tree focuses on correcting the errors made by the previous ones. Initially, all training instances are given an equal weight, but as trees are added, the algorithm increases the weights of misclassified instances, thereby forcing subsequent trees to focus more on difficult cases. This process of reweighting and adding learners continues for a predefined number of iterations, or until perfect accuracy is achieved on the training set. The final model is a weighted sum of all the weak learners, where more accurate learners have higher weights. AdaBoost is particularly known for its ability to boost the performance of simple models, its ease of implementation, and its effectiveness in reducing both bias and variance. However, it is sensitive to noisy data and outliers. Despite this, AdaBoost remains a widely used technique, especially in classification problems, due to its strong performance and simplicity.

| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 76.41%   |   76.41%  | 76.41% |  76.41%  |

## Boosting with XGBoost
Boosting with XGBoost (eXtreme Gradient Boosting) is a highly efficient and scalable implementation of gradient boosting, a machine learning technique used extensively for both regression and classification problems. XGBoost stands out for its ability to handle large and complex datasets with speed and accuracy. It operates by sequentially adding models, typically decision trees, where each new model attempts to correct the errors of the previous ones. This iterative refinement helps in capturing complex patterns in the data. XGBoost introduces several key innovations: a novel tree learning algorithm for handling sparse data, a more regularized model formalization to control over-fitting, and efficient use of computational resources. Furthermore, it includes features like built-in cross-validation and the capability to handle missing values, which enhance its robustness and flexibility. XGBoost has gained immense popularity in the machine learning community due to its performance in numerous machine learning competitions and its ability to efficiently process large datasets while achieving high performance. It is a go-to algorithm for many practitioners who require a powerful, flexible, and efficient boosting method.

| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 80.75%   |   80.75%  | 80.75% |  80.75%  |

# Classification Algorithms

Utilized different classification Algorithms


## SVM
Support Vector Machine (SVM) is a powerful and versatile supervised machine learning model, particularly renowned for its effectiveness in classification tasks. At its core, SVM seeks to find the optimal hyperplane that best separates different classes in the feature space. The beauty of SVM lies in its ability to handle both linear and non-linear data. In linear classification, it focuses on maximizing the margin between data points of different classes, with the points closest to the hyperplane (support vectors) being crucial in defining the separation. For non-linear classification, SVM employs a technique called the kernel trick, which transforms the data into a higher-dimensional space where a linear separator is possible. This makes SVM highly adaptable to various datasets. SVM models are known for their robustness, especially in cases where the number of features exceeds the number of samples. They are widely used in applications ranging from image classification to bioinformatics, due to their ability to handle complex and high-dimensional data efficiently.

### TF-IDF

| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 80.99%   |   80.99%  | 80.99% |  80.99%  |




## Multinominal Naive Bayes
The Multinomial Naive Bayes model is a popular variant of the Naive Bayes algorithm, tailored specifically for text classification tasks, particularly in scenarios where the features (typically words) are represented as frequency counts. This model operates under the core principle of Naive Bayes, which is the assumption of independence among features. In the context of text, it means that the presence or frequency of certain words in a document is independent of the presence of other words. This simplifying assumption, while often unrealistic in real-world language, enables the model to perform surprisingly well in practice, particularly in tasks like spam detection, sentiment analysis, and topic categorization. The "multinomial" aspect refers to the statistical distribution that the model assumes for the data - it treats features as arising from a multinomial distribution, which is suitable for data that can be turned into counts, such as word counts in text. Despite its simplicity and the strong independence assumption, Multinomial Naive Bayes is known for its efficiency, ease of implementation, and particularly good performance with high-dimensional data, such as large text corpora.

### TF-IDF

| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 56.66%   |   56.66%  | 56.66% |  56.66%  |



## Logistic Regression
Logistic Regression is a widely used statistical method for binary classification problems, where the goal is to predict a binary outcome (such as yes/no, win/lose, 0/1). Despite its name, it's a linear model for classification rather than regression. At its core, Logistic Regression models the probability that a given input belongs to a particular category. It uses the logistic function (or sigmoid function) to transform its output to a probability value which can then be mapped to two or more discrete classes. This function outputs a value between 0 and 1, which is interpreted as the probability of the dependent variable being a '1' (or true, success, etc.). The model coefficients (or weights) are typically learned using methods like maximum likelihood estimation. Logistic Regression is highly interpretable, computationally inexpensive, and provides probabilities for predictions, making it a fundamental tool for binary classification problems. It's particularly useful in fields such as medical diagnosis, credit scoring, and any domain where probabilistic outcomes are of interest. Despite its simplicity, it can serve as a strong baseline for binary classification problems.

### TF-IDF

| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 76.79%   |  76.79%   | 76.79% |  76.79%  |



# Deep learning



* ## BERT

BERT base model

The BERT (Bidirectional Encoder Representations from Transformers) base model represents a significant breakthrough in the field of natural language processing (NLP). Developed by Google, it stands out for its ability to understand the context of a word in a sentence by analyzing the words that come before and after it, a method known as bidirectional training. This is a departure from previous models that typically processed text in one direction, either left-to-right or right-to-left. BERT base consists of 12 layers (transformer blocks), 768 hidden units, and 12 attention heads, totaling 110 million parameters. This structure enables it to handle a wide range of NLP tasks, such as question answering, language inference, and sentiment analysis, with remarkable effectiveness. Its pre-training on a large corpus of text, followed by fine-tuning for specific tasks, allows it to achieve state-of-the-art results across various benchmarks.





| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 97.19%   | 97.19%    | 97.19% | 97.19%   |

The t-SNE visualization of BERT embeddings represents high-dimensional BERT embeddings in a 2D space, aiming to preserve the local structure of the data. Each point in the plot corresponds to an embedding from the test data. The arrangement of points reflects the similarity between embeddings; points closer together likely have similar semantic meanings or contextual relationships. The color gradient (controlled by labels_test) indicates different categories or labels, showcasing potential clustering or separation among classes in the reduced 2D space.


<img width="710" alt="Screenshot 2023-11-14 at 6 08 05 PM" src="https://github.com/adityasinha0523/CSE573-Fake-Review-Detection-Group-6/assets/78943993/f4e3e1db-ce1d-4d74-9404-f8229fd016d4">

<br />

​                   


* ## RoBERTa

 RoBERTa base model

RoBERTa (Robustly Optimized BERT Approach) is an enhanced version of the original BERT model, optimized for improved performance in natural language understanding tasks. Developed by Facebook AI, RoBERTa modifies key hyperparameters in BERT, such as training the model with larger mini-batches and learning rates, and training on more data for a longer period. Unlike BERT, RoBERTa removes the next sentence prediction objective and instead focuses solely on more extensive masked language modeling. This approach allows for deeper bidirectional context understanding. The base version of RoBERTa is similar in size to BERT base, with 12 layers, 768 hidden units, and 12 attention heads. However, it distinguishes itself by demonstrating superior performance on a range of benchmark NLP tasks. Its success can be attributed to these subtle yet impactful changes in training methodology and model architecture, which enable it to better capture nuances in language.



| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 86.99%   |  87.78%   | 98.74% |  92.94%  |

<br />

* ## Feed Forward

 Feed Forward base model

A feedforward neural network is a foundational model in the field of machine learning and artificial intelligence, characterized by its simplicity and versatility. In a feedforward model, information moves in only one direction – from input nodes, through hidden layers (if any), and finally to output nodes. There are no cycles or loops in the network, distinguishing it from more complex architectures like recurrent neural networks. The "base" version of a feedforward network typically refers to a basic configuration with a single hidden layer, although the complexity can vary. Each node in the hidden layer transforms the input via a set of learned weights and a non-linear activation function, enabling the network to capture complex relationships in the data. Feedforward networks are widely used in tasks like classification and regression, serving as a fundamental building block for more advanced neural network architectures. Their straightforward structure makes them an excellent starting point for understanding neural network operations.

With 4 layers and 500 epochs:

| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 87.86%   |  87.86%   | 87.86% | 87.86%   |

With 10 layers and 40000 epochs:

| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 93.14%   |  93.14%   | 93.14% | 93.14%   |

<br />

* ## XLMBERT

XLMBERT  model

XLMBERT (Cross-Lingual Masked Language Model using BERT) is an advanced variant of the original BERT model, designed to excel in cross-lingual understanding and tasks. It extends the capabilities of BERT to multiple languages, enabling the model to understand and process text across different linguistic contexts. This is achieved by pre-training the model on a large, diverse corpus of text spanning multiple languages. Unlike BERT, which focuses on understanding language in a largely monolingual context, XLMBERT leverages a cross-lingual masked language model training, which allows it to predict missing words in a sentence not only in one language but across different languages. This enhances its ability to understand the nuances and contexts of words in multiple languages simultaneously. XLMBERT's architecture typically follows the transformer-based design of BERT, but it is fine-tuned to handle the complexities and subtleties of multilingual processing. As a result, XLMBERT is particularly effective in tasks that require understanding and processing information across different languages, such as machine translation, multilingual classification, and cross-lingual information retrieval.





| Accuracy | Precision | Recall | F1-Score |
| -------- | --------- | ------ | -------- |
| 86.99%   |  87.78%   | 98.74% |  92.94%  |

<br />
